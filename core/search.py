import numpy as np
import spacy
from sentence_transformers import SentenceTransformer

nlp = spacy.load("ru_core_news_sm")
model = SentenceTransformer("all-MiniLM-L6-v2")

SECTION_ALIASES = {
    "—Å–æ–æ–±—â–µ–Ω–∏–µ": "section_6",
    "—Å–æ–æ–±—â–µ–Ω–∏—è": "section_6",
    "–ø–µ—Ä–µ–ø–∏—Å–∫–∞": "section_6",
    "–æ—Ç–ø—Ä–∞–≤–∏—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ": "section_6",
    "—Ç–µ—Ö–ø–æ–¥–¥–µ—Ä–∂–∫–∞": "section_2",
    "–∑–∞—è–≤–∫–∞ –≤ —Ç–µ—Ö–ø–æ–¥–¥–µ—Ä–∂–∫—É": "section_2",
    "–∞–Ω–∫–µ—Ç–∞": "section_8",
    "–∞–Ω–∫–µ—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ": "section_8",
    "–ø—Ä–æ—Ñ–∏–ª—å": "section_9",
    "—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ—Ñ–∏–ª—å": "section_9",
    "–ª–∏—á–Ω—ã–π –ø—Ä–æ—Ñ–∏–ª—å": "section_9",
    "–∏–∑–º–µ–Ω–∏—Ç—å –ø—Ä–æ—Ñ–∏–ª—å": "section_9",
    "–ø—Ä–æ—Ñ–∞–π–ª": "section_9",
    "–¥–æ–∫—É–º–µ–Ω—Ç—ã": "section_7",
    "—Å–∫–∞—á–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç": "section_7",
    "–Ω–æ–≤–æ—Å—Ç–∏": "section_4",
    "–±–ª–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π": "section_4",
    "–ø—É–±–ª–∏–∫—É–µ—Ç—Å—è –≤ –Ω–æ–≤–æ—Å—Ç—è—Ö": "section_4",
    "–ø—É–±–ª–∏–∫–∞—Ü–∏–∏": "section_4",
    "—á–∞—Å—Ç–æ –∑–∞–¥–∞–≤–∞–µ–º—ã–µ –≤–æ–ø—Ä–æ—Å—ã": "section_5",
    "faq": "section_5",
    "–≥–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞": "section_1",
    "—Ñ—É–Ω–∫—Ü–∏–∏ —Å–∏—Å—Ç–µ–º—ã": "section_1",
    "–¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏": "section_1"
}

def preprocess_text(text):
    doc = nlp(text.lower().strip())
    return " ".join([token.lemma_ for token in doc if not token.is_punct])

def find_best_answer(query, index, texts, metadata,
                     section_index, section_metadata,
                     keyword_index, keywords_metadata,
                     top_k=5, debug=False):
    query_proc = preprocess_text(query)
    query_vec = np.array(model.encode(query_proc), dtype=np.float32)

    # üîç –ü–æ–∏—Å–∫ –ø–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞–º
    D_text, I_text = index.search(query_vec.reshape(1, -1), top_k * 5)

    # üìå –ê–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö —Ä–∞–∑–¥–µ–ª–æ–≤ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º (–ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è)
    query_lemmas = set(query_proc.split())
    section_scores = {}
    section_densities = {}
    for meta in section_metadata:
        sec_id = meta["section"]
        keywords = keywords_metadata.get(sec_id, [])
        norm_keywords = set(preprocess_text(" ".join(keywords)).split())
        match_count = sum(1 for kw in norm_keywords if any(kw in lemma or lemma in kw for lemma in query_lemmas))
        density = match_count / len(norm_keywords) if norm_keywords else 0.0
        section_scores[sec_id] = match_count
        section_densities[sec_id] = density

    # üß† –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Å–∏–Ω–æ–Ω–∏–º–∞–º (–ª–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ)
    best_section = None
    for alias, sec_id in SECTION_ALIASES.items():
        alias_lemmas = preprocess_text(alias)
        if alias_lemmas in query_proc:
            best_section = sec_id
            if debug:
                print(f"üìå –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω —Ä–∞–∑–¥–µ–ª –ø–æ —Å–∏–Ω–æ–Ω–∏–º—É: '{alias}' ‚Üí {sec_id}")
            break

    # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ —Å–∏–Ω–æ–Ω–∏–º—É ‚Äî –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–ª–æ—Ç–Ω–æ—Å—Ç—å –∏ —Å—á—ë—Ç
    if not best_section:
        best_section = max(section_densities.items(), key=lambda x: (x[1], section_scores[x[0]]), default=(None, 0))[0]

    if debug:
        print(f"üîç –ê–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –ø–æ —Ä–∞–∑–¥–µ–ª–∞–º:")
        for s_id in section_scores:
            print(f"   –†–∞–∑–¥–µ–ª {s_id} ‚Äî {section_scores[s_id]} —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π, –ø–ª–æ—Ç–Ω–æ—Å—Ç—å: {section_densities[s_id]:.2f}")
        print(f"üìå –í—ã–±—Ä–∞–Ω –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–π —Ä–∞–∑–¥–µ–ª: {best_section}")

    candidates = []
    for score, i in zip(D_text[0], I_text[0]):
        section = metadata[i]['section']
        text = texts[i]

        # üöÄ –†–∞—Å—à–∏—Ä—è–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã, –µ—Å–ª–∏ –Ω–∞–¥–æ
        extended_text = text
        if len(text.split()) < 20:
            if i + 1 < len(texts) and metadata[i + 1]['section'] == section:
                extended_text += " " + texts[i + 1]

        boost = 0.3 if section == best_section else 0.0
        kw_score = section_scores.get(section, 0) * 0.05  # –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —É—Å–∏–ª–µ–Ω–∏–µ
        final_score = score + boost + kw_score

        candidates.append((extended_text, final_score, section))

    # üß† –ñ–µ—Å—Ç–∫–∏–π —Ñ–∏–ª—å—Ç—Ä –ø–æ –≤—ã–±—Ä–∞–Ω–Ω–æ–º—É —Ä–∞–∑–¥–µ–ª—É
    filtered = [c for c in candidates if c[2] == best_section]
    final = filtered if filtered else candidates

    final.sort(key=lambda x: -x[1])
    best_chunks = [c[0] for c in final[:top_k]]

    if debug:
        print("üìå –í—ã–±—Ä–∞–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã:")
        for i, chunk in enumerate(best_chunks):
            print(f"{i+1}.", chunk[:100], "...")

    return best_chunks
