from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
from difflib import SequenceMatcher

_tokenizer = None
_model = None

def get_phi2():
    global _tokenizer, _model
    if _tokenizer is None or _model is None:
        print("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –º–æ–¥–µ–ª—å Phi-2 –∏–∑ ./models/phi2 ...")
        _tokenizer = AutoTokenizer.from_pretrained("microsoft/phi-2", cache_dir="./models/phi2")
        _model = AutoModelForCausalLM.from_pretrained("microsoft/phi-2", cache_dir="./models/phi2")
        _model.to("cpu")
        print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
    return _tokenizer, _model

def rewrite_answer(question: str, context: str) -> str:
    tokenizer, model = get_phi2()

    prompt = (
        f"–í–æ–ø—Ä–æ—Å: {question}\n"
        f"–ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}\n"
        f"–û—Ç–≤–µ—Ç:"
    )

    try:
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True).to("cpu")
        outputs = model.generate(
            **inputs,
            max_new_tokens=60,
            do_sample=True,
            temperature=0.7,
            top_k=50,
            top_p=0.95,
            num_return_sequences=1
        )
        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)

        if "–û—Ç–≤–µ—Ç:" in generated:
            result = generated.split("–û—Ç–≤–µ—Ç:", 1)[1].strip()
        else:
            result = generated[len(prompt):].strip()

        similarity = SequenceMatcher(None, result, context).ratio()

        print("\n--- –û—Ç–ª–∞–¥–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ ---")
        print("[PROMPT]:", prompt)
        print("[RAW]:", generated)
        print("[RESULT]:", result)
        print(f"[SIMILARITY]: {similarity:.2f}")
        print("------------------------\n")

        if similarity > 0.9:
            return f"–í—ã –º–æ–∂–µ—Ç–µ {context[0].lower() + context[1:]}"

        return result

    except Exception as e:
        return f"[–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å Phi-2: {e}]"
