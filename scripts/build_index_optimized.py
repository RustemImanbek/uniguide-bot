import os
from langchain_community.document_loaders import DirectoryLoader, UnstructuredMarkdownLoader
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import OllamaEmbeddings
from langchain.docstore.document import Document
from keybert import KeyBERT

INDEX_DIR = "faiss_index"
DOCS_DIR = "data/rag_docs"
MODEL_NAME = "mistral"  # –∏–ª–∏ bge-small, nomic, –∏ —Ç.–¥.

kw_model = KeyBERT(model='all-MiniLM-L6-v2')


def enrich_keywords_if_missing(docs):
    enriched = 0
    for doc in docs:
        content = doc.page_content.lower()
        if "–∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞:" not in content:
            keywords = kw_model.extract_keywords(doc.page_content, top_n=5)
            tags = ", ".join([kw[0] for kw in keywords])
            doc.page_content += f"\n\n–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {tags}"
            enriched += 1
    print(f"üß† –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–æ–ø–æ–ª–Ω–µ–Ω–æ –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏: {enriched}")
    return docs


def load_documents():
    loader = DirectoryLoader(
        DOCS_DIR,
        glob="**/*.md",
        loader_cls=UnstructuredMarkdownLoader,
        show_progress=True
    )
    raw_docs = loader.load()
    raw_docs = enrich_keywords_if_missing(raw_docs)
    return raw_docs


def split_documents(documents):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=700,
        chunk_overlap=100
    )
    return splitter.split_documents(documents)


def build_faiss_index():
    print("üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –æ–±–æ–≥–∞—â–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã...")
    raw_docs = load_documents()
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(raw_docs)}")

    chunks = split_documents(raw_docs)
    print(f"‚úÇÔ∏è –ß–∞–Ω–∫–æ–≤ –ø–æ—Å–ª–µ —Ä–∞–∑–±–∏–µ–Ω–∏—è: {len(chunks)}")

    print("üîó –ü–æ–¥–∫–ª—é—á–∞–µ–º embedding –º–æ–¥–µ–ª—å...")
    embeddings = OllamaEmbeddings(model=MODEL_NAME)

    print("üß± –°—Ç—Ä–æ–∏–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º FAISS –∏–Ω–¥–µ–∫—Å...")
    vectorstore = FAISS.from_documents(chunks, embeddings)
    vectorstore.save_local(INDEX_DIR)
    print(f"‚úÖ –ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤: {INDEX_DIR}")


if __name__ == "__main__":
    build_faiss_index()
