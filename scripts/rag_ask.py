from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain_community.llms import Ollama

from langchain_core.prompts import PromptTemplate
from langchain_core.documents import Document

import re

# 1. Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ embedding
print("ðŸ¤– Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ MiniLM embedding...")
embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# 2. Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ FAISS Ð¸Ð½Ð´ÐµÐºÑ
print("ðŸ“‚ Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ FAISS Ð¸Ð½Ð´ÐµÐºÑ...")
db = FAISS.load_local("faiss_index", embedding, allow_dangerous_deserialization=True)

# 3. ÐÐ°ÑÑ‚Ñ€Ð°Ð¸Ð²Ð°ÐµÐ¼ LLM
llm = Ollama(model="mistral")

# 4. Retrieval QA
qa = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=db.as_retriever(search_kwargs={"k": 10}),
    return_source_documents=True
)

print("âœ… Ð“Ð¾Ñ‚Ð¾Ð²Ð¾! Ð’Ð²ÐµÐ´Ð¸Ñ‚Ðµ Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ð¸Ð»Ð¸ 'exit' Ð´Ð»Ñ Ð²Ñ‹Ñ…Ð¾Ð´Ð°.\n")

while True:
    query = input("ðŸ”¹ Ð’Ð¾Ð¿Ñ€Ð¾Ñ: ")
    if query.lower() in ["exit", "Ð²Ñ‹Ñ…Ð¾Ð´"]:
        break

    result = qa.invoke({"query": query})

    docs = result.get("source_documents", [])
    answer = result.get("result", "").strip()

    if not docs:
        print("âš ï¸ ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð½Ð°Ð¹Ñ‚Ð¸ Ñ€ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½Ñ‹Ðµ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ‹.")
        continue

    print("\nðŸ“š ÐšÐ¾Ð½Ñ‚ÐµÐºÑÑ‚, Ð¿ÐµÑ€ÐµÐ´Ð°Ð½Ð½Ñ‹Ð¹ Ð² Ð¼Ð¾Ð´ÐµÐ»ÑŒ:\n")
    for i, doc in enumerate(docs, 1):
        text = doc.page_content.strip()
        preview = text[:1000] + ("..." if len(text) > 1000 else "")
        print(f"--- Ð”Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚ {i} ---")
        print(preview)
        print()

    print("ðŸ’¬ ÐžÑ‚Ð²ÐµÑ‚:\n", answer, "\n")
