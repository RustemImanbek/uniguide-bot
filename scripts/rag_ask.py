from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import DirectoryLoader
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.llms import Ollama
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA

import os

# 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
print("\U0001F4E5 –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã...")
loader = DirectoryLoader(path="data/rag_docs", glob="**/*.md")
docs = loader.load()
print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(docs)}")

# 2. –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
chunks = splitter.split_documents(docs)
print(f"‚úÇÔ∏è –ß–∞–Ω–∫–æ–≤ –ø–æ–ª—É—á–µ–Ω–æ: {len(chunks)}")

# 3. –ü–æ–ª—É—á–∞–µ–º embedding —á–µ—Ä–µ–∑ Ollama
print("\U0001F916 –ü–æ–ª—É—á–∞–µ–º embedding —á–µ—Ä–µ–∑ Ollama...")
embedding = OllamaEmbeddings(model="mistral")

# 4. –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–ª–∏ –∑–∞–≥—Ä—É–∂–∞–µ–º FAISS
index_path = "faiss_index"

if os.path.exists(index_path):
    print("\U0001F4E6 –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å FAISS...")
    vectorstore = FAISS.load_local(index_path, embedding, allow_dangerous_deserialization=True)
else:
    print("\U0001F9F1 –°—Ç—Ä–æ–∏–º –∏–Ω–¥–µ–∫—Å FAISS...")
    vectorstore = FAISS.from_documents(chunks, embedding)
    vectorstore.save_local(index_path)
    print("‚úÖ –ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω!")

# 5. –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º LLM + Retrieval Chain
print("\U0001F9E0 –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º LLM –∏ RetrievalQA...")
llm = Ollama(model="mistral")
qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=vectorstore.as_retriever())

# 6. –î–∏–∞–ª–æ–≥
print("\n‚úÖ –ì–æ—Ç–æ–≤–æ! –í–≤–µ–¥–∏—Ç–µ –≤–æ–ø—Ä–æ—Å –∏–ª–∏ 'exit' –¥–ª—è –≤—ã—Ö–æ–¥–∞.")
while True:
    question = input("\nüîπ –í–æ–ø—Ä–æ—Å: ")
    if question.lower() in ["exit", "quit"]:
        print("üëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
        break

    # üß† –ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞ –≤–æ–ø—Ä–æ—Å–∞ (–ø—Ä–∏–º–∏—Ç–∏–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞)
    lowered = question.lower()
    if any(keyword in lowered for keyword in ["faq", "–≤–æ–ø—Ä–æ—Å—ã", "—á–∞—Å—Ç–æ –∑–∞–¥–∞–≤–∞–µ–º—ã–µ", "–ø–æ–¥–¥–µ—Ä–∂–∫–∞"]):
        question += " –ì–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è —Ä–∞–∑–¥–µ–ª —Å —á–∞—Å—Ç–æ –∑–∞–¥–∞–≤–∞–µ–º—ã–º–∏ –≤–æ–ø—Ä–æ—Å–∞–º–∏ (FAQ) –≤ —Å–∏—Å—Ç–µ–º–µ?"

    # üì° –ü–æ–ª—É—á–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞ —Å –æ—Ü–µ–Ω–∫–∞–º–∏
    results = vectorstore.similarity_search_with_score(question, k=10)
    filtered_docs = [doc for doc, score in results if score > 0.7]

    if not filtered_docs:
        print("\n‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.")
        continue

    print("\nüìö –ö–æ–Ω—Ç–µ–∫—Å—Ç, –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–π –≤ –º–æ–¥–µ–ª—å:")
    for i, (doc, score) in enumerate(results, 1):
        print(f"\n--- –î–æ–∫—É–º–µ–Ω—Ç {i} (score={score:.2f}) ---")
        print(doc.page_content[:1000])

    print("\nü§ñ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞...")
    response = qa_chain.invoke({"query": question})
    print(f"\nüí¨ –û—Ç–≤–µ—Ç:\n{response['result']}")